

@ARTICLE{1994adams,
  author={R. {Adams} and L. {Bischof}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Seeded region growing},
  year={1994},
  volume={16},
  number={6},
  pages={641-647},
  keywords={image segmentation;seeded region growing;intensity image segmentation;seed selection;Image segmentation;Surface fitting;Surface morphology;Surface topography;Pixel;Filters;Mathematics;Statistics;Australia;Robustness},
  doi={10.1109/34.295913},
  ISSN={1939-3539},
  month={6},}

@INPROCEEDINGS{2011szenasi,
author={S. {Szénási} and Z. {Vámossy} and M. {Kozlovszky}},
booktitle={2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI)},
title={GPGPU-based data parallel region growing algorithm for cell nuclei detection},
year={2011},
volume={},
number={},
pages={493-499},
keywords={biological tissues;cellular biophysics;graphics processing units;medical image processing;parallel architectures;GPGPU based data parallel region growing algorithm;cell nuclei detection;microscopic analysis;tissue samples;digital imagery;immunodiagnostic software;hematoxylin eosin stained cell nuclei;location identification;size identification;region growing method;CUDA implementation;Graphics processing unit;Kernel;Instruction sets;Computer architecture;Algorithm design and analysis;Microprocessors;Synchronization;GPGPU;CUDA;data parallel algorithm;biomedical image processing;nuclei detection},
doi={10.1109/CINTI.2011.6108556},
ISSN={null},
month={11},}

@article{2018melouah,
  author    = {Ahlem Melouah and
               Soumia Layachi},
  title     = {Overview of automatic seed selection methods for biomedical images
               segmentation},
  journal   = {Int. Arab J. Inf. Technol.},
  volume    = {15},
  number    = {3},
  pages     = {499--504},
  year      = {2018},
  url       = {http://iajit.org/index.php?option=com\_content\&task=blogcategory\&id=130\&Itemid=455},
  timestamp = {Wed, 27 Mar 2019 13:18:45 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/iajit/MelouahL18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{2014park,
author={Park, Seongjin
and Lee, Jeongjin
and Lee, Hyunna
and Shin, Juneseuk
and Seo, Jinwook
and Lee, Kyoung Ho
and Shin, Yeong-Gil
and Kim, Bohyoung},
title={Parallelized seeded region growing using CUDA},
journal={Computational and mathematical methods in medicine},
year={2014},
publisher={Hindawi Publishing Corporation},
volume={2014},
pages={856453-856453},
keywords={Algorithms},
keywords={Colon/diagnostic imaging},
keywords={Computer Graphics},
keywords={Computer Systems},
keywords={Computers},
keywords={Humans},
keywords={Image Enhancement/methods},
keywords={Lung/diagnostic imaging},
keywords={Programming Languages},
keywords={Radiographic Image Interpretation, Computer-Assisted},
keywords={Software},
keywords={Tomography, X-Ray Computed/*methods},
abstract={This paper presents a novel method for parallelizing the seeded region growing (SRG) algorithm using Compute Unified Device Architecture (CUDA) technology, with intention to overcome the theoretical weakness of SRG algorithm of its computation time being directly proportional to the size of a segmented region. The segmentation performance of the proposed CUDA-based SRG is compared with SRG implementations on single-core CPUs, quad-core CPUs, and shader language programming, using synthetic datasets and 20 body CT scans. Based on the experimental results, the CUDA-based SRG outperforms the other three implementations, advocating that it can substantially assist the segmentation during massive CT screening tests.},
note={25309619[pmid]},
note={PMC4189527[pmcid]},
issn={1748-6718},
doi={10.1155/2014/856453},
url={https://www.ncbi.nlm.nih.gov/pubmed/25309619}
}

@mastersthesis{2011lorentzen,
author={Lorentzen, Erlend Andreas},
title={Fast Seeded Region Growing in a 3D Grid},
year={2011},
publisher={Norwegian University of Science and Technology},
url={https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/252779},
}
@InProceedings{2019real,
author="Real, P.
and Molina-Abril, H.
and D{\'i}az-del-R{\'i}o, F.
and Blanco-Trejo, S.
and Onchis, D.",
editor="Carrasco-Ochoa, Jes{\'u}s Ariel
and Mart{\'i}nez-Trinidad, Jos{\'e} Francisco
and Olvera-L{\'o}pez, Jos{\'e} Arturo
and Salas, Joaqu{\'i}n",
title="Enhanced Parallel Generation of Tree Structures for the Recognition of 3D Images",
booktitle="Pattern Recognition",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="292--301",
abstract="Segmentations of a digital object based on a connectivity criterion at n-xel or sub-n-xel level are useful tools in image topological analysis and recognition. Working with cell complex analogous of digital objects, an example of this kind of segmentation is that obtained from the combinatorial representation so called Homological Spanning Forest (HSF, for short) which, informally, classifies the cells of the complex as belonging to regions containing the maximal number of cells sharing the same homological (algebraic homology with coefficient in a field) information. We design here a parallel method for computing a HSF (using homology with coefficients in {\$}{\$}{\backslash}mathbb {\{}Z{\}}/2{\backslash}mathbb {\{}Z{\}}{\$}{\$}) of a 3D digital object. If this object is included in a 3D image of {\$}{\$}m{\_}1{\backslash}times m{\_}2 {\backslash}times m{\_}3{\$}{\$}voxels, its theoretical time complexity order is near {\$}{\$}O(log(m{\_}1+m{\_}2+m{\_}3)){\$}{\$}, under the assumption that a processing element is available for each voxel. A prototype implementation validating our results has been written and several synthetic, random and medical tridimensional images have been used for testing. The experiments allow us to assert that the number of iterations in which the homological information is found varies only to a small extent from the theoretical computational time.",
isbn="978-3-030-21077-9"
}

@article{2004chang,
title = "A linear-time component-labeling algorithm using contour tracing technique",
journal = "Computer Vision and Image Understanding",
volume = "93",
number = "2",
pages = "206 - 220",
year = "2004",
issn = "1077-3142",
doi = "https://doi.org/10.1016/j.cviu.2003.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S1077314203001401",
author = "Fu Chang and Chun-Jen Chen and Chi-Jen Lu",
keywords = "Component-labeling algorithm, Contour tracing, Linear-time algorithm",
abstract = "A new linear-time algorithm is presented in this paper that simultaneously labels connected components (to be referred to merely as components in this paper) and their contours in binary images. The main step of this algorithm is to use a contour tracing technique to detect the external contour and possible internal contours of each component, and also to identify and label the interior area of each component. Labeling is done in a single pass over the image, while contour points are revisited more than once, but no more than a constant number of times. Moreover, no re-labeling is required throughout the entire process, as it is required by other algorithms. Experimentation on various types of images (characters, half-tone pictures, photographs, newspaper, etc.) shows that our method outperforms methods that use the equivalence technique. Our algorithm not only labels components but also extracts component contours and sequential orders of contour points, which can be useful for many applications."
}

@article{2017he,
title = "The connected-component labeling problem: A review of state-of-the-art algorithms",
journal = "Pattern Recognition",
volume = "70",
pages = "25 - 43",
year = "2017",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2017.04.018",
url = "http://www.sciencedirect.com/science/article/pii/S0031320317301693",
author = "Lifeng He and Xiwei Ren and Qihang Gao and Xiao Zhao and Bin Yao and Yuyan Chao",
keywords = "Connected-component labeling, Shape feature, Image analysis, Image understanding, Pattern recognition, Computer vision",
abstract = "This article addresses the connected-component labeling problem which consists in assigning a unique label to all pixels of each connected component (i.e., each object) in a binary image. Connected-component labeling is indispensable for distinguishing different objects in a binary image, and prerequisite for image analysis and object recognition in the image. Therefore, connected-component labeling is one of the most important processes for image analysis, image understanding, pattern recognition, and computer vision. In this article, we review state-of-the-art connected-component labeling algorithms presented in the last decade, explain the main strategies and algorithms, present their pseudo codes, and give experimental results in order to bring order of the algorithms. Moreover, we will also discuss parallel implementation and hardware implementation of connected-component labeling algorithms, extension for n-D images, and try to indicate future work on the connected component labeling problem."
}


@Article{gremse2016,
author={Gremse, Felix
and St{\"a}rk, Marius
and Ehling, Josef
and Menzel, Jan Robert
and Lammers, Twan
and Kiessling, Fabian},
title={Imalytics Preclinical: Interactive Analysis of Biomedical Volume Data},
journal={Theranostics},
year={2016},
month={01},
day={01},
publisher={Ivyspring International Publisher},
volume={6},
number={3},
pages={328-341},
keywords={GPU Processing; Interactive Segmentation; Medical Image Analysis; Multimodal Imaging; Segmentation Rendering; Undo/Redo; Animals; Humans; Image Processing, Computer-Assisted/*methods; Multimodal Imaging/*methods; *Software},
abstract={A software tool is presented for interactive segmentation of volumetric medical data sets. To allow interactive processing of large data sets, segmentation operations, and rendering are GPU-accelerated. Special adjustments are provided to overcome GPU-imposed constraints such as limited memory and host-device bandwidth. A general and efficient undo/redo mechanism is implemented using GPU-accelerated compression of the multiclass segmentation state. A broadly applicable set of interactive segmentation operations is provided which can be combined to solve the quantification task of many types of imaging studies. A fully GPU-accelerated ray casting method for multiclass segmentation rendering is implemented which is well-balanced with respect to delay, frame rate, worst-case memory consumption, scalability, and image quality. Performance of segmentation operations and rendering are measured using high-resolution example data sets showing that GPU-acceleration greatly improves the performance. Compared to a reference marching cubes implementation, the rendering was found to be superior with respect to rendering delay and worst-case memory consumption while providing sufficiently high frame rates for interactive visualization and comparable image quality. The fast interactive segmentation operations and the accurate rendering make our tool particularly suitable for efficient analysis of multimodal image data sets which arise in large amounts in preclinical imaging studies.},
note={26909109[pmid]},
note={PMC4737721[pmcid]},
note={thnov06p0328[PII]},
issn={1838-7640},
doi={10.7150/thno.13624},
url={https://pubmed.ncbi.nlm.nih.gov/26909109},
url={https://doi.org/10.7150/thno.13624},
language={eng}
}

@book{patterson2013,
author = {Patterson, David A. and Hennessy, John L.},
title = {Computer Organization and Design, Fifth Edition: The Hardware/Software Interface},
year = {2013},
isbn = {0124077269},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {5th},
abstract = {The 5th edition of Computer Organization and Design moves forward into the post-PC era with new examples, exercises, and material highlighting the emergence of mobile computing and the cloud. This generational change is emphasized and explored with updated content featuring tablet computers, cloud infrastructure, and the ARM (mobile computing devices) and x86 (cloud computing) architectures. Because an understanding of modern hardware is essential to achieving good performance and energy efficiency, this edition adds a new concrete example, "Going Faster," used throughout the text to demonstrate extremely effective optimization techniques. Also new to this edition is discussion of the "Eight Great Ideas" of computer architecture. As with previous editions, a MIPS processor is the core used to present the fundamentals of hardware technologies, assembly language, computer arithmetic, pipelining, memory hierarchies and I/O. Instructors looking for4th Edition teaching materials should e-mail textbook@elsevier.com. Includes new examples, exercises, and material highlighting the emergence of mobile computing and the Cloud. Covers parallelism in depth with examples and content highlighting parallel hardware and software topics Features the Intel Core i7, ARM Cortex-A8 and NVIDIA Fermi GPU as real-world examples throughout the book Adds a new concrete example, "Going Faster," to demonstrate how understanding hardware can inspire software optimizations that improve performance by 200 times. Discusses and highlights the "Eight Great Ideas" of computer architecture: Performance via Parallelism; Performance via Pipelining; Performance via Prediction; Design for Moore's Law; Hierarchy of Memories; Abstraction to Simplify Design; Make the Common Case Fast; and Dependability via Redundancy. Includes a full set of updated and improved exercises.}
}
@article{allegretti2019,
	number={2},
	volume={31},
	issn={1045-9219},
	isbn={},
	source_code={https://github.com/prittt/YACCLAB},
	doi={https://doi.org/10.1109/TPDS.2019.2934683},
	note={},
	publisher={IEEE},
	venue={},
	month={08},
	year={2019},
	pages={423--438},
	journal={IEEE Transactions on Parallel and Distributed Systems},
	title={{Optimized Block-Based Algorithms to Label Connected Components on GPUs}},
	author={Allegretti, Stefano and Bolelli, Federico and Grana, Costantino},
}

@inproceedings{alipour2017,
author = {Alipour, Mehdi and Carlson, Trevor E. and Kaxiras, Stefanos},
title = {Exploring the Performance Limits of Out-of-Order Commit},
year = {2017},
isbn = {9781450344876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3075564.3075581},
doi = {10.1145/3075564.3075581},
abstract = {Out-of-order execution is essential for high performance, general-purpose computation, as it can find and execute useful work instead of stalling. However, it is limited by the requirement of visibly sequential, atomic instruction execution --- in other words in-order instruction commit. While in-order commit has its advantages, such as providing precise interrupts and avoiding complications with the memory consistency model, it requires the core to hold on to resources (reorder buffer entries, load/store queue entries, registers) until they are released in program order. In contrast, out-of-order commit releases resources much earlier, yielding improved performance with fewer traditional hardware resources. However, out-of-order commit is limited in terms of correctness by the conditions described in the work of Bell and Lipasti. In this paper we revisit out-of-order commit from a different perspective, not by proposing another hardware technique, but by examining these conditions one by one and in combination with respect to their potential performance benefit for both non-speculative and speculative out-of-order commit. While correctly handling recovery for all out-of-order commit conditions currently requires complex tracking and expensive checkpointing, this work aims to demonstrate the potential for selective, speculative out-of-order commit using an oracle implementation without speculative rollback costs. We learn that: a) there is significant untapped potential for aggressive variants of out-of-order commit; b) it is important to optimize the commit depth, or the search distance for out-of-order commit, for a balanced design: smaller cores can benefit from shorter depths while larger cores continue to benefit from aggressive parameters; c) the focus on a subset of out-of-order commit conditions could lead to efficient implementations; d) the benefits for out-of-order commit increase with higher memory latency and works well in conjunction with prefetching to continue to improve performance.},
booktitle = {Proceedings of the Computing Frontiers Conference},
pages = {211–220},
numpages = {10},
location = {Siena, Italy},
series = {CF'17}
}

@inproceedings{vila2020,
author = {Vila, Pepe and Ganty, Pierre and Guarnieri, Marco and K\"{o}pf, Boris},
title = {CacheQuery: Learning Replacement Policies from Hardware Caches},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386008},
doi = {10.1145/3385412.3386008},
abstract = {We show how to infer deterministic cache replacement policies using off-the-shelf automata learning and program synthesis techniques. For this, we construct and chain two abstractions that expose the cache replacement policy of any set in the cache hierarchy as a membership oracle to the learning algorithm, based on timing measurements on a silicon CPU. Our experiments demonstrate an advantage in scope and scalability over prior art and uncover two previously undocumented cache replacement policies.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–532},
numpages = {14},
keywords = {Automata Learning, Cache Replacement Policies, Reverse Engineering, Program Synthesis},
location = {London, UK},
series = {PLDI 2020}
}

@InProceedings{adhi2019,
author="Adhi, Boma A.
and Mase, Masayoshi
and Hosokawa, Yuhei
and Kishimoto, Yohei
and Onishi, Taisuke
and Mikami, Hiroki
and Kimura, Keiji
and Kasahara, Hironori",
editor="Rauchwerger, Lawrence",
title="Software Cache Coherent Control by Parallelizing Compiler",
booktitle="Languages and Compilers for Parallel Computing",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="17--25",
abstract="Recently multicore technology has enabled development of hundreds or thousands core processor on a single chip. However, on such multicore processor, cache coherence hardware will become very complex, hot and expensive. This paper proposes a parallelizing compiler directed software coherence scheme for shared memory multicore systems without hardware cache coherence control. The general idea of the proposed method is that an automatic parallelizing compiler parallelize coarse grain task, analyzes stale data and line sharing in the program, then solves those problems by simple program restructuring and data synchronization. The proposed method is a simple and efficient software cache coherent control scheme built on OSCAR automatic parallelizing compiler and evaluated on Renesas RP2 with 8 SH-4A cores processor. The cache coherence hardware on the RP2 processor is only available for up to 4 cores. The cache coherence hardware can also be turned off for non-coherence cache mode. Performance evaluation was performed using 10 benchmark programs from SPEC2000, SPEC2006, NAS Parallel Benchmark (NPB) and MediaBench II. The proposed method performed as good as or better than hardware cache coherence scheme while still provided correct result as the hardware coherent mechanism. For example, the proposed software cache coherent control (NCC) gave us 2.63 times speedup for SPEC 2000 equake with 4 cores against sequential execution while got only 2.52 times speedup for 4 cores MESI hardware coherent control. Also, the software coherence control gave us 4.37 speed up for 8 cores with no hardware coherent mechanism available.",
isbn="978-3-030-35225-7"
}

@CONFERENCE{bailey1991,
author = {Donald G. Bailey},
title = {Raster Based Region Growing},
booktitle = {6th New Zealand Image Processing Workshop},
year = {1991}
}

@Article{dong2020,
author={Dong, Yunyun
and Yang, Wenkai
and Wang, Jiawen
and Zhao, Zijuan
and Wang, Sanhu
and Cui, Qiang
and Qiang, Yan},
title={An improved supervoxel 3D region growing method based on PET/CT multimodal data for segmentation and reconstruction of GGNs},
journal={Multimedia Tools and Applications},
year={2020},
month={01},
day={01},
volume={79},
number={3},
pages={2309-2338},
abstract={Among the various types of lung nodules, ground glass nodules (GGNs) are difficult to segment accurately due to complex morphological characteristics. Moreover, GGNs are associated with a higher malignancy probability. Three-dimensional (3D) segmentation and reconstruction techniques can help physicians intuitively elucidate the relationship between lung nodules and their surrounding tissues. We propose an improved supervoxel 3D region growing approach based on positron emission tomography/computed tomography (PET/CT) multimodal data for the segmentation and reconstruction of GGNs. First, the seed point is automatically located with PET information and a 3D mask is generated. Then, a fuzzy connectivity (FC) map is generated based on the 3D mask, and an improved supervoxel 3D region growing is utilized on a fuzzy connectivity map under the constraints of the 3D mask. Finally, 3D GGNs segmentation and reconstruction results are obtained. Qualitative and quantitative comparisons between our proposed method and other region growing methods shows great superiority of our proposed method, with the Jaccard similarity coefficient between our proposed method and physician manual segmentation reaching 95.61{\%}; the average processing time is 16.38 s. Experimental results show that our proposed supervoxel-based 3D region growing method is very promising for assisting physicians in diagnosis.},
issn={1573-7721},
doi={10.1007/s11042-019-08250-4},
url={https://doi.org/10.1007/s11042-019-08250-4}
}

@Inbook{al-manasia2015,
author="Al-Manasia, Malik
and Chaczko, Zenon",
editor="Borowik, Grzegorz
and Chaczko, Zenon
and Jacak, Witold
and {\L}uba, Tadeusz",
title="Evaluation of Cache Coherence Mechanisms for Multicore Processors",
bookTitle="Computational Intelligence and Efficiency in Engineering Systems",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="307--320",
abstract="Multiple core designs have become commonplace in the processor marketplace,
and are therefore a major focus in modern computer architecture research. Thus, for both
product development and research, multiple core processor performance evaluation is a
mandatory step in marketplace. Multicore computing has presented many challenges for
system designers; one of which is data consistency between a shared cache or memory and
the local caches of the chip. This is also known as cache coherency. The cache coherence
mechanisms are a key component in the direction of accomplishing the goal of continuing
exponential performance growth through widespread thread-level parallelism. In the scope
of this research, we have studied the available efficient methods and protocols used to
achieve cache coherence in multicore architectures. These protocols were further modeled
and evaluated utilizing Simics simulator for multicore architectures. We also explored the
weaknesses and strengths of different protocols and discussed the way of improving
them.",
isbn="978-3-319-15720-7",
doi="10.1007/978-3-319-15720-7_22",
url="https://doi.org/10.1007/978-3-319-15720-7_22"
}
